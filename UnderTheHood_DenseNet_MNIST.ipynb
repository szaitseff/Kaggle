{
  "cells": [
    {
      "metadata": {
        "_uuid": "65069ba70a42dec77013a5278dddeb1efff9df92"
      },
      "cell_type": "markdown",
      "source": "# 0. Description"
    },
    {
      "metadata": {
        "_uuid": "741b7c196e4b7d053ac7a08bbe2f4379a13a2922"
      },
      "cell_type": "markdown",
      "source": "Hi! I concieved of the ___Under the Hood___ series of Kaggle kernels as part of my machine learning training. The idea is to implement popular machine learning models in Python/Numpy and compare their performance against a benchmark, i.e. a similar model in the machine learning libraries like Scikit-learn or Keras. I agree with Andrej Karpathy who wrote in his [blog](http://karpathy.github.io/neuralnets/) : \"...everything became much clearer when I started writing code.\" \n\nThis kernel is dedicated to the ___Dense Neural Network___ (aka fully connected network) applied to the __MNIST__ dataset. We have the following __challenges__: \nfor an adjustable network architecture implement model with He initialization, forward propagation with ReLU activation and dropout regularization in the hidden layers and Softmax activation in the output layer, cross-entropy cost function with L2 regularization, backpropagation and parameters update with stochastic gradient descent and Adam optimizer, and finally prediction of classificaton labels after learning the model for various hyperparameters. If compared to the __Titanic__ dataset, there are more features (pixels) in the input, and therefore we can try larger hidden layers size (512 in this kernel).\n \nAny feedback or ideas are welcome."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Load necessary libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt  # data visualization\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical # to convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Dropout\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.regularizers import l2",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "dbf552bd663d81348eee7859bf5a54b3ef48162e"
      },
      "cell_type": "markdown",
      "source": "## 1. MNIST DATA"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d7512437115d5f0e0f06de283dc0f34cb3e9107a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Load datasets\ntrain, test = pd.read_csv(\"../input/train.csv\"), pd.read_csv(\"../input/test.csv\")",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f60b8ede01f0ac1d54e80c9579849a3295354d92"
      },
      "cell_type": "code",
      "source": "# Review data\nprint(f'train data shape = {train.shape}', '/', f'test data shape = {test.shape}')\ntrain.head()",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "train data shape = (42000, 785) / test data shape = (28000, 784)\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0      1       0       0       0       0       0       0       0       0   \n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n\n   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n0       0    ...            0         0         0         0         0   \n1       0    ...            0         0         0         0         0   \n2       0    ...            0         0         0         0         0   \n3       0    ...            0         0         0         0         0   \n4       0    ...            0         0         0         0         0   \n\n   pixel779  pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0         0  \n1         0         0         0         0         0  \n2         0         0         0         0         0  \n3         0         0         0         0         0  \n4         0         0         0         0         0  \n\n[5 rows x 785 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 785 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "68dd04b9c99fea496ff5d8bf555a8e491fab76ac"
      },
      "cell_type": "code",
      "source": "# let's check the count of different labels in the dataset (~balanced)\ntrain['label'].value_counts()",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "1    4684\n7    4401\n3    4351\n9    4188\n2    4177\n6    4137\n0    4132\n4    4072\n8    4063\n5    3795\nName: label, dtype: int64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6b06102607ec76c9516c2782823afbbe951788c5"
      },
      "cell_type": "code",
      "source": "# Numpy representation of the train and test data:\ntrain_pixels, test_pixels = train.iloc[:,1:].values.astype('float32'), test.values.astype('float32') # all pixel values\ntrain_labels = train.iloc[:,0].values.astype('int32') # only labels i.e targets digits\ntrain_labels = train_labels.reshape(-1, 1) # ensure proper shape of the array\n\nprint(f'train_pixels shape = {train_pixels.shape}')\nprint(f'test_pixels shape = {test_pixels.shape}')\nprint(f'train_labels shape = {train_labels.shape}')",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "train_pixels shape = (42000, 784)\ntest_pixels shape = (28000, 784)\ntrain_labels shape = (42000, 1)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "960c6a5b844995fe12aa74d548893785d1d4382d"
      },
      "cell_type": "code",
      "source": "# Reshape input data to fit Keras model (height=28px, width=28px, channels=1):\ntrain_pixels, test_pixels = train_pixels.reshape(-1, 28, 28, 1), test_pixels.reshape(-1, 28, 28, 1)\nprint(f'train_pixels shape = {train_pixels.shape}')\nprint(f'test_pixels shape = {test_pixels.shape}')",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "train_pixels shape = (42000, 28, 28, 1)\ntest_pixels shape = (28000, 28, 28, 1)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6caf10e515af9325cc2ed5715c65cdf53fc8f42a",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Visualize some images from the dataset:\nnrows, ncols = 3, 5  # number of rows and colums in subplots\nfig, ax = plt.subplots(nrows, ncols, sharex=True, sharey=True, figsize=(8,5))\nfor row in range(nrows):\n    for col in range(ncols):\n        i = np.random.randint(0, 30000)  # pick up arbitrary examples\n        ax[row, col].imshow(train_pixels[i,:,:,0], cmap='Greys')\n        ax[row, col].set_title(f'<{train.label[i]}>');",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<matplotlib.figure.Figure at 0x7faf892ce0b8>",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAE/CAYAAAB1i6tsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "56c89842676b38c6a589ebafec48748765c3dfcf",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Input data are greyscale pixels of intensity [0:255]. Let's normalize to [0:1]:\ntrain_pixels, test_pixels = train_pixels / 255.0, test_pixels / 255.0",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b1b60ab298ae8fd9e7a5ed6564516c2a89e3733a"
      },
      "cell_type": "code",
      "source": "# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\ntrain_labels = to_categorical(train_labels, num_classes = 10)\nprint(f'train_labels shape = {train_labels.shape}')\ntrain_labels",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "train_labels shape = (42000, 10)\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "array([[0., 1., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 1., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 1.]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6cd9b63d7053c8415b08740d73f5f44cab6b2ca7"
      },
      "cell_type": "code",
      "source": "# Split training and validation set for the fitting\ntrain_pixels, val_pixels, train_labels, val_labels = train_test_split(train_pixels, train_labels, test_size = 0.1, random_state=2)\n\ntrain_pixels.shape, train_labels.shape, val_pixels.shape, val_labels.shape, test_pixels.shape",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "((37800, 28, 28, 1),\n (37800, 10),\n (4200, 28, 28, 1),\n (4200, 10),\n (28000, 28, 28, 1))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c5169f7d244c26c8200b08efe656c2a437cb096a"
      },
      "cell_type": "code",
      "source": "# let's fix the important numbers for further modeling:\nm_train = train_pixels.shape[0]   # number of examples in the training set\nm_val = val_pixels.shape[0]       # number of examples in the validation set\nm_test = test_pixels.shape[0]     # number of examples in the test set\nn_x = test.shape[1]               # input size, number of pixels in the image\nn_y = train_labels.shape[1]       # output size, number of label classes\nprint(f\" m_train = {m_train} / m_val = {m_val} / m_test = {m_test} / n_x = {n_x} / n_y = {n_y}\")",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": " m_train = 37800 / m_val = 4200 / m_test = 28000 / n_x = 784 / n_y = 10\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "01a1775d75c696511bc0c49c3c6f8471a9a8da4f"
      },
      "cell_type": "markdown",
      "source": "## 2. A DENSE NEURAL NETWORK IN KERAS"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c7f3917252f075f8ffe60f7c43ce4dede1d5bf9b",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Decide on the model architecture: [n_x, hidden_layers, n_y]\nlayer_dims = [n_x, 512, n_y]  # the model architecture is adjustable",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dc83c21b61ea3b543d1b6f018e0f2095ec7f3f3a"
      },
      "cell_type": "code",
      "source": "# create an instance of a neural network:\nk_model = Sequential()\n# the first hidden layer must have input dimensions:\nk_model.add(Flatten(input_shape=[28,28,1]))\nk_model.add(Dense(layer_dims[1], activation='relu',\n                  kernel_regularizer=l2(0)))\nk_model.add(Dropout(0.25))\n# additional hidden layers are optional\n# output layer w/softmax activation:\nk_model.add(Dense(n_y, activation='softmax',\n                  kernel_regularizer=l2(0)))\n\n# Compile the model w/Adam optimizer:\nk_model.compile(optimizer=Adam(lr=1e-2),\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])\n\n# Define a learning rate decay method:\nlr_decay = ReduceLROnPlateau(monitor='loss', \n                             patience=1, verbose=0, \n                             factor=0.5, min_lr=1e-6)\n# Train the model:\nk_model.fit(train_pixels, train_labels, epochs=30, batch_size=128,\n            callbacks=[lr_decay], verbose=0)\n\n# Evaluate the model:\nk_train_loss, k_train_acc = k_model.evaluate(train_pixels, train_labels)\nk_val_loss, k_val_acc = k_model.evaluate(val_pixels, val_labels)\n\nprint(f'k_model: train accuracy = {round(k_train_acc * 100, 4)}%')\nprint(f'k_model: val accuracy = {round(k_val_acc * 100, 4)}%')\nprint(f'k_model: val error = {round((1 - k_val_acc) * m_val)} examples')\n",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": "37800/37800 [==============================] - 2s 62us/step\n4200/4200 [==============================] - 0s 62us/step\nk_model: train accuracy = 99.9894%\nk_model: val accuracy = 98.1905%\nk_model: val error = 76.0 examples\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "26043c058ef21042c6904b13f67eae090b1dde54"
      },
      "cell_type": "markdown",
      "source": "## 3. A CUSTOM NEURAL NETWORK IN PYTHON/NUMPY (classification)"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "f97ea283c78102ddf6cce0ee8acc0695e40585d4"
      },
      "cell_type": "markdown",
      "source": "Let's have a look \"under the hood\" at a similar model in Python/Numpy and appreciate the greatness of Keras :)"
    },
    {
      "metadata": {
        "_uuid": "03a534e0682b05f855ba4f6f7daf64199c04e07e"
      },
      "cell_type": "markdown",
      "source": "### 3.1 Reshape data and define mini-batches"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bfec56b49c612bead60b5ea36fab2bba84cf84d7"
      },
      "cell_type": "code",
      "source": "# Reshape data to fit the Custom Model architecture: (n_pixels, m_examples)\nX_train = train_pixels.reshape(m_train, -1).T\nY_train = train_labels.T\nX_val   = val_pixels.reshape(m_val,-1).T\nY_val   = val_labels.T\nX_test  = test_pixels.reshape(m_test, -1).T\n\nX_train.shape, Y_train.shape, X_val.shape, Y_val.shape, X_test.shape",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "((784, 37800), (10, 37800), (784, 4200), (10, 4200), (784, 28000))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a33497c7eb7b156483f9f19580a76a28bc430819",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Define a function to create random mini-batches for the gradient descent:\ndef random_mini_batches(X, Y, batch_size):\n    \"\"\"\n    This funcion creates a list of random minibatches from (X, Y)\n    Arguments:\n        X -- input data, of shape (input size, number of examples)\n        Y -- \"true\" labels vector, of shape (output size, number of examples)\n        batch_size -- size of mini-batches, integer\n    Returns:\n        mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    m = X.shape[1]              # number of examples\n    mini_batches = []           # initialize a list to contain all minibatches\n        \n    # Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation]\n\n    # Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = m // batch_size # number of minibatches of size batch_size\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k*batch_size:(k+1)*batch_size]\n        mini_batch_Y = shuffled_Y[:, k*batch_size:(k+1)*batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m_train % batch_size != 0:\n        mini_batch_X = shuffled_X[:, num_complete_minibatches*batch_size:]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches*batch_size:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a78c676ec752b9fe400a4965a4e5f55f3bcf5de7"
      },
      "cell_type": "markdown",
      "source": "### 3.2 Define a neural network model with fully connected layers"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d6c44b43902f8d7225ebc28f151f9c749fb2e97e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "class Custom_model(object):\n    \n    def __init__(self, layer_dims):\n        \"\"\" \n        The model consists of the input layer (pixels), a number of hidden layers and\n        the output layer (categorical classifier). To create an instance of the model, we set\n        dimensions of its layers and initialize parameters for the hidden/ output layers.\n        Arguments: \n            layer_dims -- list containing the input size and each layer size\n        Returns: \n            parameters -- python dictionary containing parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                bl -- bias vector of shape (layer_dims[l], 1) \n        \"\"\"\n        self.layer_dims = layer_dims       # a list with dimensions of all layers\n        self.num_layers = len(layer_dims)  # number of layers (with input layer)\n        self.parameters = {}        # a dictionary with weights and biases of the model\n        # Initializing weights randomly (He initialization) and biases to zeros\n        for l in range(1, len(layer_dims)):\n            self.parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], \n                                                       layer_dims[l-1])*np.sqrt(2./layer_dims[l-1])\n            self.parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n    \n    # define getters and setters for accessing the model class attributes:        \n    def get_layer_dims(self):\n        return self.layer_dims\n    def get_num_layers(self):\n        return self.num_layers\n    def get_params(self, key):\n        return self.parameters.get(key)\n    def set_params(self, key, value):\n        self.parameters[key] = value\n    \n        \n    def forward_propagation(self, X, keep_prob):\n        \"\"\" \n        Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation\n        Arguments:\n            X -- data, numpy array of shape (input size, number of examples)\n            keep_prob - probability of keeping a neuron active during drop-out, scalar\n        Returns:\n            AL -- last post-activation value\n            caches -- list of caches containing:\n               every cache of layers w/ReLU activation (there are L-1 of them, indexed from 0 to L-2)\n               the cache of the output layer with Softmax activation (there is one, indexed L-1)\n        \"\"\"\n        caches = []\n        L = self.get_num_layers()-1    # number of layers with weights (hidden + output)\n        A = X                          # set input as the first hidden layer activation\n        # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n        for l in range(1, L):\n            A_prev = A                # initialize activation of the previous layer\n            W, b = self.get_params(f'W{l}'), self.get_params(f'b{l}') # get weights and biases\n            Z = W.dot(A_prev) + b     # linear activation for the hidden layers\n            A = np.maximum(0,Z)       # ReLU activation for the hidden layers\n            if keep_prob == 1:        # if no dropout\n                cache = (A_prev, Z)   # useful during backpropagation\n            elif keep_prob < 1:       # if dropout is used for regularization\n                D = np.random.rand(A.shape[0], A.shape[1])  # initialize matrix D\n                D = D < keep_prob   # convert entries of D to 0/1 (using keep_prob as threshold)\n                A *= D              # shut down some neurons of A\n                A /= keep_prob      # scale the value of neurons that haven't been shutdown\n                cache = (A_prev, Z, D)   # useful during backpropagation\n            caches.append(cache)\n        # Implement LINEAR -> SOFTMAX. Add \"cache\" to the \"caches\" list.\n        W, b = self.get_params(f'W{L}'), self.get_params(f'b{L}')\n        Z = W.dot(A) + b                        # Linear activation of the output layer\n        Z -= np.max(Z, axis=0, keepdims=True)   # Normalize Z to make Softmax stable\n        AL = np.exp(Z)/np.sum(np.exp(Z),axis=0,\n                              keepdims=True) # Softmax activation of the output layer\n        cache = (A, Z)                          # useful during backpropagation\n        caches.append(cache)\n        return AL, caches\n        \n    \n    def compute_cost(self, AL, Y, lambd):\n        \"\"\"\n        Implement the cost function with L2 regularization.\n        Arguments:\n            AL -- post-activation, output of forward propagation,\n                                    of shape (output size, number of examples)\n            Y -- \"true\" labels vector, of shape (output size, number of examples)\n            lambd -- regularization hyperparameter, scalar\n        Returns:\n            cost - value of the regularized loss function\n        \"\"\"\n        m = AL.shape[1]               # number of training examples\n        L = self.get_num_layers()-1   # number of layers with weights (hidden and output)\n        assert(Y.shape == AL.shape)\n        # Compute the cross-entropy part of the cost for Softmax activation function:\n        cross_entropy_cost = -(1./m) * np.sum(np.multiply(Y, np.log(AL)))\n\n        # Compute L2 regularization cost\n        L2_reg_cost = 0\n        if lambd == 0:\n            pass\n        else:\n            for l in range(1, L+1):     # sum of all squared weights\n                L2_reg_cost += (1./m)*(lambd/2)*(np.sum(np.square(self.get_params(f'W{l}'))))\n\n        # Total cost:\n        cost = cross_entropy_cost + L2_reg_cost\n        # To make sure cost's shape is what we expect (e.g. this turns [[17]] into 17).\n        cost = np.squeeze(cost)   \n        return cost\n    \n            \n    def backward_propagation(self, AL, Y, caches, lambd, keep_prob):\n        \"\"\" \n        Implement the backward propagation for the [LINEAR->RELU]*(L-1)->[LINEAR->SOFTMAX]\n        Arguments:\n            AL -- probability vectors for the training examples, output of the forward propagation\n            Y -- true one-hot \"label\" vectors for the training examples\n            caches -- list of caches containing:\n                every cache of forward propagation with \"relu\" \n                                        (there are (L-1) or them, indexes from 0 to L-2)\n                the cache of forward propagation with \"softmax\" (there is one, index L-1)\n            lambd -- lambda, an L2 regularization parameter, scalar\n            keep_prob - probability of keeping a neuron active during drop-out, scalar\n        Returns: grads -- a dictionary with updated gradients\n        \"\"\"\n        L = self.get_num_layers()-1  # number of layers with weights (hidden and output)\n        m = AL.shape[1]              # number of training examples\n        assert(Y.shape == AL.shape)\n        grads = {}                   # a dict for the gradients of the cost function\n\n        # Lth layer (SOFTMAX -> LINEAR) gradients.\n        W = self.get_params(f\"W{L}\") # get weights for the output layer\n        if keep_prob == 1:           # without dropout\n            A_prev, Z = caches[L-1]  # get inputs and linear activations for the output layer\n        elif keep_prob < 1:          # with dropout \n            A_prev, Z, D = caches[L-1]  # get inputs, linear activations and mask\n        dZ = AL - Y                  # Gradient of the cost w.r.t. Z (from calculus)\n        dW = (1./m) * dZ.dot(A_prev.T) + (lambd/m) * W  # Gradient of cost w.r.t. W\n        db = (1./m) * np.sum(dZ, axis=1, keepdims=True) # Gradient of cost w.r.t. b\n        dA_prev = np.dot(W.T,dZ)                        # Gradient of cost w.r.t. dA_prev\n        if keep_prob == 1:           # without dropout\n            pass\n        elif keep_prob < 1:          # with dropout\n            dA_prev *= D   # apply mask D to shutdown the same neurons as during forward prop\n            dA_prev /= keep_prob  # scale the value of neurons that haven't been shut down\n        # Update the grads dictionary for the output layer\n        grads[f\"dA{L-1}\"] = dA_prev\n        grads[f\"dW{L}\"], grads[f\"db{L}\"] = dW, db\n\n        # l-th hidden layer: (RELU -> LINEAR) gradients.\n        for l in reversed(range(1, L)):\n            W = self.get_params(f\"W{l}\") # get weights for the l-th hidden layer\n            if keep_prob == 1:           # without dropout\n                A_prev, Z = caches[l-1]  # get ReLU & linear activations for l-th hidden layer\n            elif keep_prob < 1:          # with dropout\n                A_prev, Z, D = caches[l-1]    # get ReLU & linear activations + mask\n            dZ = np.array(dA_prev, copy=True) # just converting dz to a correct object.\n            dZ[Z <= 0] = 0                    # when z <= 0, set dz to 0 as well. \n            dW = (1./m) * dZ.dot(A_prev.T) + (lambd/m) * W  # Gradient of cost w.r.t. W\n            db = (1./m) * np.sum(dZ, axis=1, keepdims=True) # Gradient of cost w.r.t. b\n            dA_prev = np.dot(W.T,dZ)          # Gradient of cost w.r.t. dA_prev\n            if keep_prob == 1:                # without dropout\n                pass\n            elif keep_prob < 1:               # with dropout\n                dA_prev *= D  # apply mask to shutdown the same neurons as during forward prop\n                dA_prev /= keep_prob # scale the value of neurons that haven't been shut down\n            # Update the grads dictionary for the hidden layers\n            grads[f\"dA{l-1}\"] = dA_prev \n            grads[f\"dW{l}\"], grads[f\"db{l}\"] = dW, db\n        \n        return grads\n    \n        \n    def update_parameters_with_gd(self, grads, lr):\n        \"\"\" \n        Update parameters of the model using method gradient descent\n        Arguments:\n            grads -- python dictionary containing gradients, output of backprop\n            lr -- the learning rate, scalar\n        Returns: \n            updated parameters (weithts and biases of the model)\n        \"\"\"\n        L = self.get_num_layers()-1  # get number of layers with weights (hidden + output)\n        for l in range(1, L+1):\n            self.set_params(f\"W{l}\", self.get_params(f\"W{l}\") - lr * grads[f\"dW{l}\"])\n            self.set_params(f\"b{l}\", self.get_params(f\"b{l}\") - lr * grads[f\"db{l}\"])\n               \n            \n    def initialize_adam(self):\n        \"\"\"\n        Initializes v and s for the Adam optimizer as two python dictionaries with:\n            keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n            values: numpy arrays of zeros of the same shape as the corresponding gradients.\n        Returns: \n            v -- python dict that will contain the exponentially weighted average of the gradient.\n            s -- python dict that will contain the exponentially weighted average of the squared gradient.\n        \"\"\"\n        v = {}   \n        s = {}\n        L = self.get_num_layers()-1   # get number of layers with weights (hidden and output)\n        for l in range(1, L+1):\n            v[f\"dW{l}\"] = np.zeros((self.get_params(f\"W{l}\").shape))\n            v[f\"db{l}\"] = np.zeros((self.get_params(f\"b{l}\").shape))\n            s[f\"dW{l}\"] = np.zeros((self.get_params(f\"W{l}\").shape))\n            s[f\"db{l}\"] = np.zeros((self.get_params(f\"b{l}\").shape))    \n        return v,s\n    \n    \n    def update_parameters_with_adam(self, grads, v, s, t, lr, beta1, beta2, epsilon):\n        \"\"\"\n        Update parameters of the model using Adam optimization algorithm\n        Arguments:\n            v -- Adam variable, moving average of the first gradient, python dict\n            s -- Adam variable, moving average of the squared gradient, python dict\n            t -- current timestep (minibatch)\n            lr -- the learning rate, scalar.\n            beta1 -- Exponential decay hyperparameter for the first moment estimates \n            beta2 -- Exponential decay hyperparameter for the second moment estimates \n            epsilon -- hyperparameter preventing division by zero in Adam updates\n        Returns:\n            updated parameters (model attributes)\n            v -- Adam variable, moving average of the first gradient, python dict\n            s -- Adam variable, moving average of the squared gradient, python dict\n        \"\"\"\n        v_corr = {}       # Initializing a bias-corrected first moment\n        s_corr = {}       # Initializing a bias corrected second moment estimate\n        L = self.get_num_layers()-1   # get number of layers with weights (hidden+output)\n        \n        # Perform Adam update on all parameters\n        for l in range(1, L+1):\n            # Moving average of the gradients\n            v[f\"dW{l}\"] = beta1*v[f\"dW{l}\"] + (1-beta1)*grads[f\"dW{l}\"]\n            v[f\"db{l}\"] = beta1*v[f\"db{l}\"] + (1-beta1)*grads[f\"db{l}\"]\n\n            # Compute bias-corrected first moment estimate\n            v_corr[f\"dW{l}\"] = v[f\"dW{l}\"]/(1-beta1**t)\n            v_corr[f\"db{l}\"] = v[f\"db{l}\"]/(1-beta1**t)\n\n            # Moving average of the squared gradients\n            s[f\"dW{l}\"] = beta2*s[f\"dW{l}\"] + (1-beta2)*grads[f\"dW{l}\"]**2\n            s[f\"db{l}\"] = beta2*s[f\"db{l}\"] + (1-beta2)*grads[f\"db{l}\"]**2\n\n            # Compute bias-corrected second raw moment estimate\n            s_corr[f\"dW{l}\"] = s[f\"dW{l}\"]/(1-beta2**t)\n            s_corr[f\"db{l}\"] = s[f\"db{l}\"]/(1-beta2**t)\n\n            # Update parameters\n            temp_W = v_corr[f\"dW{l}\"]/(s_corr[f\"dW{l}\"]**0.5 + epsilon)\n            temp_b = v_corr[f\"db{l}\"]/(s_corr[f\"db{l}\"]**0.5 + epsilon)\n            self.set_params(f\"W{l}\", self.get_params(f\"W{l}\") - lr*temp_W)\n            self.set_params(f\"b{l}\", self.get_params(f\"b{l}\") - lr*temp_b)\n\n        return v, s\n        \n        \n    def predict(self, X):\n        \"\"\" \n        This method is used to predict the results of a L-layer neural network.\n        Arguments:\n            X -- dataset of examples to label, shape (num_pixels, num_examples)\n        Returns:\n            p -- predictions of one-hot labels for X, shape (num_classes, num_examples)\n        \"\"\"\n        m = X.shape[1]          # number of examples\n        # Forward propagation\n        AL, _ = self.forward_propagation(X, keep_prob=1)\n        n = AL.shape[0]         # number of classes\n        p = np.zeros((n, m))    # initialize predictions\n        # convert probabilities AL to one-hot label predictions:\n        for i in range(m):\n            max_i = np.amax(AL[:,i], axis=0)\n            for j in range(n):\n                if AL[j,i] == max_i:\n                    p[j,i] = 1\n                else:\n                    p[j,i] = 0\n        return np.int64(p)\n    \n        \n    def fit(self, X, Y, batch_size, num_epochs, lr, min_lr=1e-6, lambd=0, keep_prob=1,\n            beta1=0.9, beta2=0.999, epsilon=1e-8, optimizer='adam', print_cost=False):\n        \"\"\" \n        Implements an L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX.\n        Arguments:\n            X -- data, numpy array of shape (# pixels, # examples)\n            Y -- true \"label\" vector, of shape (# classes, # examples)\n            batch_size -- the size of a mini batch on which parameters are get updated\n            num_epochs -- number of epochs, i.e. passes through the training set\n            lr -- learning rate of the gradient descent update rule\n            min_lr -- the lower threshold of the learning rate decay\n            lambd -- lambda, the L2 regularization hyperparameter\n            optimizer -- optimization metod [\"gd\"=gradient_descent or \"adam\"]\n            beta1 -- exp decay hyperparameter for the past gradients estimates in 'adam'\n            beta2 -- exp decay hyperparameter for the past squared gradients estimates in 'adam'\n            epsilon -- hyperparameter preventing division by zero in 'adam' updates\n            print_cost -- if True, it prints the cost every # steps\n        Returns:\n            parameters -- parameters learnt by the model. They can then be used to predict.\n        \"\"\"\n        costs = []                # to keep track of the cost\n        lr_0 = lr                 # to fix the initial learning rate before decay\n        t = 0                     # initializing the minibatch counter (for Adam update)\n        cost_prev = 1000          # initialize cost to a big number\n        # Initialize the optimizer\n        if optimizer == \"gd\":\n            pass                  # no initialization required for gradient descent\n        elif optimizer == \"adam\":\n            v, s = self.initialize_adam()\n        # Optimization loop\n        for epoch in range(num_epochs):\n            # Define the random minibatches. Reshuffle the dataset after each epoch\n            minibatches = random_mini_batches(X, Y, batch_size)\n            for minibatch in minibatches:\n                # Select a minibatch\n                (minibatch_X, minibatch_Y) = minibatch\n                # Forward propagation: [LINEAR->RELU]*(L-1) -> LINEAR->SOFTMAX.\n                AL, caches = self.forward_propagation(minibatch_X, keep_prob)\n                # Compute cost.\n                cost = self.compute_cost(AL, minibatch_Y, lambd)\n                # Backward propagation.\n                grads = self.backward_propagation(AL, minibatch_Y, caches, lambd, keep_prob)\n                # Update parameters\n                t = t + 1 # minibatch counter\n                if optimizer == \"gd\":\n                    self.update_parameters_with_gd(grads, lr)\n                elif optimizer == \"adam\":\n                    v, s = self.update_parameters_with_adam(grads, v, s, t, lr, \n                                                            beta1, beta2, epsilon)\n                # Define learning rate decay:\n                if cost > cost_prev and lr > min_lr:\n                    lr = lr / 2   # reduce lr, but not below the min value\n                cost_prev = cost  # save cost value for the next iteration\n                \n            # Print the cost every 20 epoch\n            if print_cost and epoch % 20 == 0:\n                print(f\"Cost after epoch {epoch}: {cost}\")\n            if print_cost and epoch % 1 == 0:    \n                costs.append(cost)\n        # plot the Learning curve\n        if print_cost:\n            plt.plot(np.squeeze(costs))\n            plt.ylabel(\"cost\")\n            plt.xlabel(\"epochs (x 1)\")\n            plt.title(f\"Learning rate = {lr_0} / {lr}\")\n            plt.show()",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "83a4cde7a4f46ae7a3ac924bdc584a434e425c9c"
      },
      "cell_type": "markdown",
      "source": "### **4. MODEL TRAINING  **"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8d8743a90ed9f9bf593386bdc41c338987cb2491",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Create list to save journal records of the current session\nrecords_list = []",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1831854f8d5da69d4f7351b42df457e17dd13cb3"
      },
      "cell_type": "code",
      "source": "# Create an instance of the Custom_model class with 'layer_dims' architecture:\nc_model = Custom_model(layer_dims)\n\n# Set hyperparameters that we want to tune for the custom model:\nlr          = 1e-3    # the learning rate for the gradient descent\nmin_lr      = 1e-8    # the lower threshold of the learning rate decay\noptimizer   = 'adam'\nbatch_size  = m_train  \nnum_epochs  = 300          \nlambd       = 0       # lambda - regularization hyperparameter, scalar\nkeep_prob   = 0.5    # keep_prob - probability of keeping a neuron active during dropout\n\n# Train the model at various hyperparameters settings:\nc_model.fit(X_train, Y_train, batch_size=batch_size, num_epochs=num_epochs,\n            lr=lr, min_lr=min_lr, lambd=lambd, optimizer=optimizer,\n            print_cost=True)       \n\n# Evaluation on the train data:\npredict_train = c_model.predict(X_train)\ncorrect_train = np.argmax(predict_train, axis=0) == np.argmax(Y_train, axis=0)\nc_train_acc = round(np.sum(correct_train)/m_train, 2)\n\n# Evaluation on the validation data:\npredict_val = c_model.predict(X_val)\ncorrect_val = np.argmax(predict_val, axis=0) == np.argmax(Y_val, axis=0)\nc_val_acc = round(np.sum(correct_val)/m_val, 2)\n\nprint(f\"c_model: train accuracy: {c_train_acc * 100}%\")\nprint(f\"c_model: val accuracy = {c_val_acc * 100}%\")\nprint(f'c_model: val error = {round((1 - c_val_acc) * m_val)} examples')\n\n# Update the journal of hyperparameters tuning records\nrecord = {'layer_dims'   : c_model.get_layer_dims(), \n          'acc_train'    : c_train_acc, \n          'acc_val'      : c_val_acc,\n          'val_error'    : round((1 - c_val_acc) * m_val),\n          'batch_size'   : batch_size,\n          'num_epochs'   : num_epochs,\n          'lr'           : lr,\n          'min_lr'       : min_lr,\n          'lambda'       : lambd,\n          'keep_prob'    : keep_prob}\n\nrecords_list.append(record)\njournal = pd.DataFrame(records_list)   # saves records when repeatedly running the current cell\njournal",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Cost after epoch 0: 2.3387291205305765\nCost after epoch 20: 0.3984680863255955\nCost after epoch 40: 0.26603308181857244\nCost after epoch 60: 0.20921134167049113\nCost after epoch 80: 0.17118254075006814\nCost after epoch 100: 0.1419138958988666\nCost after epoch 120: 0.1186052346055381\nCost after epoch 140: 0.09977415854553706\nCost after epoch 160: 0.08436915694386227\nCost after epoch 180: 0.07158805673583453\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<matplotlib.figure.Figure at 0x7faf89085828>",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "c_model: train accuracy: 99.0%\nc_model: val accuracy = 97.0%\nc_model: val error = 126.0 examples\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "   acc_train  acc_val  batch_size  keep_prob  lambda      layer_dims  \\\n0       1.00     0.97       37800        NaN     0.0  [784, 512, 10]   \n1       0.98     0.96       37800       0.75     0.0  [784, 512, 10]   \n2       0.99     0.97       37800       0.50     0.0  [784, 512, 10]   \n\n   learning_rate     lr        min_lr  num_epochs  val_error  \n0          0.001    NaN  1.000000e-08         300      126.0  \n1            NaN  0.001  1.000000e-08         200      168.0  \n2            NaN  0.001  1.000000e-08         200      126.0  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>acc_train</th>\n      <th>acc_val</th>\n      <th>batch_size</th>\n      <th>keep_prob</th>\n      <th>lambda</th>\n      <th>layer_dims</th>\n      <th>learning_rate</th>\n      <th>lr</th>\n      <th>min_lr</th>\n      <th>num_epochs</th>\n      <th>val_error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.00</td>\n      <td>0.97</td>\n      <td>37800</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>[784, 512, 10]</td>\n      <td>0.001</td>\n      <td>NaN</td>\n      <td>1.000000e-08</td>\n      <td>300</td>\n      <td>126.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.98</td>\n      <td>0.96</td>\n      <td>37800</td>\n      <td>0.75</td>\n      <td>0.0</td>\n      <td>[784, 512, 10]</td>\n      <td>NaN</td>\n      <td>0.001</td>\n      <td>1.000000e-08</td>\n      <td>200</td>\n      <td>168.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.99</td>\n      <td>0.97</td>\n      <td>37800</td>\n      <td>0.50</td>\n      <td>0.0</td>\n      <td>[784, 512, 10]</td>\n      <td>NaN</td>\n      <td>0.001</td>\n      <td>1.000000e-08</td>\n      <td>200</td>\n      <td>126.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "ea90786fab38d2045db67a32cc44389a535572ba"
      },
      "cell_type": "markdown",
      "source": "## Conclusion: we have looked \"under the hood\" of a fully connected network with ReLU activation and dropout regularization for the hidden layers, Softmax activation for the output layer, cross-entropy cost with L2 regularization, and parameters update with Adam optimization. \n\n## Great! We know how it works!"
    },
    {
      "metadata": {
        "_uuid": "18fba4fb764536a58f7dfdca196468e161e3e245"
      },
      "cell_type": "markdown",
      "source": "# -1. Submitting Predictions to Kaggle"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "839c4e62066c4de54fae39f8c74db7964e911d5f"
      },
      "cell_type": "code",
      "source": "# Predict labels on the test dataset with the Custom model:\npredict_test = c_model.predict(X_test)   # one-hot prediction vectors\npredictions = np.argmax(predict_test, axis=0)  # label predictions\n\nsubmission = pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                           \"Label\"  : predictions})\nsubmission.to_csv(\"submission.csv\", index=False, header=True)\nsubmission.head()",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "   ImageId  Label\n0        1      2\n1        2      0\n2        3      9\n3        4      9\n4        5      3",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ImageId</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}