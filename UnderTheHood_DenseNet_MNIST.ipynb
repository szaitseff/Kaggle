{
  "cells": [
    {
      "metadata": {
        "_uuid": "65069ba70a42dec77013a5278dddeb1efff9df92"
      },
      "cell_type": "markdown",
      "source": "## 0. DESCRIPTION"
    },
    {
      "metadata": {
        "_uuid": "741b7c196e4b7d053ac7a08bbe2f4379a13a2922"
      },
      "cell_type": "markdown",
      "source": "Hi! I concieved of the ___Under the Hood___ series of Kaggle kernels as part of my machine learning training. The idea is to implement popular machine learning models in Python/Numpy and compare their performance against a benchmark, i.e. a similar model in the machine learning libraries like Scikit-learn or Keras. I agree with Andrej Karpathy who wrote in his [blog](http://karpathy.github.io/neuralnets/) : \"...everything became much clearer when I started writing code.\" \n\nThis kernel is dedicated to the ___Dense Neural Network___ (aka fully connected network) applied to the __MNIST__ dataset. What I'd like to have in the model: \n- an adjustable network architecture,\n- \"He\" initialization, \n- forward propagation with ReLU activation in the hidden layers,\n- optional dropout regularization,\n- Softmax activation in the output layer,\n- cross-entropy cost function with optional L2 regularization, \n- backpropagation and parameters update with stochastic gradient descent,\n- optional Adam optimizer, \n- prediction of classificaton labels after learning the model for various hyperparameters. \n\nIf compared to the __Titanic__ dataset, there are more features (pixels) in the input, and therefore we can try larger hidden layers size (512 in this kernel).\n \nAny feedback or ideas are welcome."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Load necessary libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt  # data visualization\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical # to convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Dropout\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.regularizers import l2",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "dbf552bd663d81348eee7859bf5a54b3ef48162e"
      },
      "cell_type": "markdown",
      "source": "## 1. MNIST DATA"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d7512437115d5f0e0f06de283dc0f34cb3e9107a"
      },
      "cell_type": "code",
      "source": "# Load datasets\ntrain, test = pd.read_csv(\"../input/train.csv\"), pd.read_csv(\"../input/test.csv\")",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f60b8ede01f0ac1d54e80c9579849a3295354d92"
      },
      "cell_type": "code",
      "source": "# Review data\nprint(f'train data shape = {train.shape}', '/', f'test data shape = {test.shape}')\ntrain.head()",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "train data shape = (42000, 785) / test data shape = (28000, 784)\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "   label  pixel0  pixel1    ...     pixel781  pixel782  pixel783\n0      1       0       0    ...            0         0         0\n1      0       0       0    ...            0         0         0\n2      1       0       0    ...            0         0         0\n3      4       0       0    ...            0         0         0\n4      0       0       0    ...            0         0         0\n\n[5 rows x 785 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>pixel9</th>\n      <th>pixel10</th>\n      <th>pixel11</th>\n      <th>pixel12</th>\n      <th>pixel13</th>\n      <th>pixel14</th>\n      <th>pixel15</th>\n      <th>pixel16</th>\n      <th>pixel17</th>\n      <th>pixel18</th>\n      <th>pixel19</th>\n      <th>pixel20</th>\n      <th>pixel21</th>\n      <th>pixel22</th>\n      <th>pixel23</th>\n      <th>pixel24</th>\n      <th>pixel25</th>\n      <th>pixel26</th>\n      <th>pixel27</th>\n      <th>pixel28</th>\n      <th>pixel29</th>\n      <th>pixel30</th>\n      <th>pixel31</th>\n      <th>pixel32</th>\n      <th>pixel33</th>\n      <th>pixel34</th>\n      <th>pixel35</th>\n      <th>pixel36</th>\n      <th>pixel37</th>\n      <th>pixel38</th>\n      <th>...</th>\n      <th>pixel744</th>\n      <th>pixel745</th>\n      <th>pixel746</th>\n      <th>pixel747</th>\n      <th>pixel748</th>\n      <th>pixel749</th>\n      <th>pixel750</th>\n      <th>pixel751</th>\n      <th>pixel752</th>\n      <th>pixel753</th>\n      <th>pixel754</th>\n      <th>pixel755</th>\n      <th>pixel756</th>\n      <th>pixel757</th>\n      <th>pixel758</th>\n      <th>pixel759</th>\n      <th>pixel760</th>\n      <th>pixel761</th>\n      <th>pixel762</th>\n      <th>pixel763</th>\n      <th>pixel764</th>\n      <th>pixel765</th>\n      <th>pixel766</th>\n      <th>pixel767</th>\n      <th>pixel768</th>\n      <th>pixel769</th>\n      <th>pixel770</th>\n      <th>pixel771</th>\n      <th>pixel772</th>\n      <th>pixel773</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "68dd04b9c99fea496ff5d8bf555a8e491fab76ac"
      },
      "cell_type": "code",
      "source": "# let's check the count of different labels in the dataset (~balanced)\ntrain['label'].value_counts()",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "1    4684\n7    4401\n3    4351\n9    4188\n2    4177\n6    4137\n0    4132\n4    4072\n8    4063\n5    3795\nName: label, dtype: int64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6b06102607ec76c9516c2782823afbbe951788c5"
      },
      "cell_type": "code",
      "source": "# Numpy representation of the train and test data:\ntrain_pixels, test_pixels = train.iloc[:,1:].values.astype('float32'), test.values.astype('float32') # all pixel values\ntrain_labels = train.iloc[:,0].values.astype('int32') # only labels i.e targets digits\ntrain_labels = train_labels.reshape(-1, 1) # ensure proper shape of the array\n\nprint(f'train_pixels shape = {train_pixels.shape}')\nprint(f'test_pixels shape = {test_pixels.shape}')\nprint(f'train_labels shape = {train_labels.shape}')",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "train_pixels shape = (42000, 784)\ntest_pixels shape = (28000, 784)\ntrain_labels shape = (42000, 1)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "960c6a5b844995fe12aa74d548893785d1d4382d"
      },
      "cell_type": "code",
      "source": "# Reshape input data to fit Keras model (height=28px, width=28px, channels=1):\ntrain_pixels, test_pixels = train_pixels.reshape(-1, 28, 28, 1), test_pixels.reshape(-1, 28, 28, 1)\nprint(f'train_pixels shape = {train_pixels.shape}')\nprint(f'test_pixels shape = {test_pixels.shape}')",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "train_pixels shape = (42000, 28, 28, 1)\ntest_pixels shape = (28000, 28, 28, 1)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6caf10e515af9325cc2ed5715c65cdf53fc8f42a",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Visualize some images from the dataset:\nnrows, ncols = 3, 5  # number of rows and colums in subplots\nfig, ax = plt.subplots(nrows, ncols, sharex=True, sharey=True, figsize=(8,5))\nfor row in range(nrows):\n    for col in range(ncols):\n        i = np.random.randint(0, 30000)  # pick up arbitrary examples\n        ax[row, col].imshow(train_pixels[i,:,:,0], cmap='Greys')\n        ax[row, col].set_title(f'<{train.label[i]}>');",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 576x360 with 15 Axes>",
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "56c89842676b38c6a589ebafec48748765c3dfcf"
      },
      "cell_type": "code",
      "source": "# Input data are greyscale pixels of intensity [0:255]. Let's normalize to [0:1]:\ntrain_pixels, test_pixels = train_pixels / 255.0, test_pixels / 255.0",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b1b60ab298ae8fd9e7a5ed6564516c2a89e3733a"
      },
      "cell_type": "code",
      "source": "# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\ntrain_labels = to_categorical(train_labels, num_classes = 10)\nprint(f'train_labels shape = {train_labels.shape}')\ntrain_labels",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "train_labels shape = (42000, 10)\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "array([[0., 1., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 1., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6cd9b63d7053c8415b08740d73f5f44cab6b2ca7"
      },
      "cell_type": "code",
      "source": "# Split training and validation set for the fitting\ntrain_pixels, val_pixels, train_labels, val_labels = train_test_split(train_pixels, train_labels, test_size = 0.1, random_state=2)\n\ntrain_pixels.shape, train_labels.shape, val_pixels.shape, val_labels.shape, test_pixels.shape",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "((37800, 28, 28, 1),\n (37800, 10),\n (4200, 28, 28, 1),\n (4200, 10),\n (28000, 28, 28, 1))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c5169f7d244c26c8200b08efe656c2a437cb096a"
      },
      "cell_type": "code",
      "source": "# let's fix the important numbers for further modeling:\nm_train = train_pixels.shape[0]   # number of examples in the training set\nm_val = val_pixels.shape[0]       # number of examples in the validation set\nm_test = test_pixels.shape[0]     # number of examples in the test set\nn_x = test.shape[1]               # input size, number of pixels in the image\nn_y = train_labels.shape[1]       # output size, number of label classes\nprint(f\" m_train = {m_train} / m_val = {m_val} / m_test = {m_test} / n_x = {n_x} / n_y = {n_y}\")",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": " m_train = 37800 / m_val = 4200 / m_test = 28000 / n_x = 784 / n_y = 10\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "01a1775d75c696511bc0c49c3c6f8471a9a8da4f"
      },
      "cell_type": "markdown",
      "source": "## 2. A DENSE NEURAL NETWORK IN KERAS"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c7f3917252f075f8ffe60f7c43ce4dede1d5bf9b"
      },
      "cell_type": "code",
      "source": "# Decide on the model architecture: [n_x, hidden_layers, n_y]\nlayer_dims = [n_x, 512, n_y]  # the model architecture is adjustable",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dc83c21b61ea3b543d1b6f018e0f2095ec7f3f3a"
      },
      "cell_type": "code",
      "source": "# create an instance of a neural network:\nk_model = Sequential()\n# the first hidden layer must have input dimensions:\nk_model.add(Flatten(input_shape=[28,28,1]))\nk_model.add(Dense(layer_dims[1], activation='relu',\n                  kernel_regularizer=l2(0)))\nk_model.add(Dropout(0.25))\n# additional hidden layers are optional\n# output layer w/softmax activation:\nk_model.add(Dense(n_y, activation='softmax',\n                  kernel_regularizer=l2(0)))\n\n# Compile the model w/Adam optimizer:\nk_model.compile(optimizer=Adam(lr=1e-3),\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])\n\n# Define a learning rate decay method:\nlr_decay = ReduceLROnPlateau(monitor='loss', \n                             patience=1, verbose=0, \n                             factor=0.5, min_lr=1e-8)\n# Train the model:\nk_model.fit(train_pixels, train_labels, epochs=20, batch_size=128,\n            callbacks=[lr_decay], verbose=0)\n\n# Evaluate the model:\nk_train_loss, k_train_acc = k_model.evaluate(train_pixels, train_labels)\nk_val_loss, k_val_acc = k_model.evaluate(val_pixels, val_labels)\n\nprint(f'k_model: train accuracy = {round(k_train_acc * 100, 4)}%')\nprint(f'k_model: val accuracy = {round(k_val_acc * 100, 4)}%')\nprint(f'k_model: val error = {round((1 - k_val_acc) * m_val)} examples')\n",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "37800/37800 [==============================] - 1s 36us/step\n4200/4200 [==============================] - 0s 36us/step\nk_model: train accuracy = 99.9947%\nk_model: val accuracy = 98.4286%\nk_model: val error = 66.0 examples\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "26043c058ef21042c6904b13f67eae090b1dde54"
      },
      "cell_type": "markdown",
      "source": "## 3. A CUSTOM NEURAL NETWORK IN PYTHON/NUMPY (classification)"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "f97ea283c78102ddf6cce0ee8acc0695e40585d4"
      },
      "cell_type": "markdown",
      "source": "Let's have a look \"under the hood\" at a similar model in Python/Numpy and appreciate the greatness of Keras :)"
    },
    {
      "metadata": {
        "_uuid": "03a534e0682b05f855ba4f6f7daf64199c04e07e"
      },
      "cell_type": "markdown",
      "source": "### 3.1 Reshape data and define mini-batches"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bfec56b49c612bead60b5ea36fab2bba84cf84d7"
      },
      "cell_type": "code",
      "source": "# Reshape data to fit the Custom Model architecture: (n_pixels, m_examples)\nX_train = train_pixels.reshape(m_train, -1).T\nY_train = train_labels.T\nX_val   = val_pixels.reshape(m_val,-1).T\nY_val   = val_labels.T\nX_test  = test_pixels.reshape(m_test, -1).T\n\nX_train.shape, Y_train.shape, X_val.shape, Y_val.shape, X_test.shape",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "((784, 37800), (10, 37800), (784, 4200), (10, 4200), (784, 28000))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a33497c7eb7b156483f9f19580a76a28bc430819"
      },
      "cell_type": "code",
      "source": "# Define a function to create random mini-batches for the gradient descent:\ndef random_mini_batches(X, Y, batch_size):\n    \"\"\"\n    This funcion creates a list of random minibatches from (X, Y)\n    Arguments:\n        X -- input data, of shape (input size, number of examples)\n        Y -- \"true\" labels vector, of shape (output size, number of examples)\n        batch_size -- size of mini-batches, integer\n    Returns:\n        mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    m = X.shape[1]              # number of examples\n    mini_batches = []           # initialize a list to contain all minibatches\n        \n    # Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation]\n\n    # Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = m // batch_size # number of minibatches of size batch_size\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k*batch_size:(k+1)*batch_size]\n        mini_batch_Y = shuffled_Y[:, k*batch_size:(k+1)*batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % batch_size != 0:\n        mini_batch_X = shuffled_X[:, num_complete_minibatches*batch_size:]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches*batch_size:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a78c676ec752b9fe400a4965a4e5f55f3bcf5de7"
      },
      "cell_type": "markdown",
      "source": "### 3.2 Define a neural network model with fully connected layers"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d6c44b43902f8d7225ebc28f151f9c749fb2e97e"
      },
      "cell_type": "code",
      "source": "class Custom_model(object):\n    \n    def __init__(self, layer_dims):\n        \"\"\" \n        The model consists of the input layer (pixels), a number of hidden layers and\n        the output layer (categorical classifier). To create an instance of the model, we set\n        dimensions of its layers and initialize parameters for the hidden/ output layers.\n        Arguments: \n            layer_dims -- list containing the input size and each layer size\n        Returns: \n            parameters -- python dictionary containing parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                bl -- bias vector of shape (layer_dims[l], 1) \n        \"\"\"\n        self.layer_dims = layer_dims       # a list with dimensions of all layers\n        self.num_layers = len(layer_dims)  # number of layers (with input layer)\n        self.parameters = {}        # a dictionary with weights and biases of the model\n        # Initializing weights randomly (He initialization) and biases to zeros\n        for l in range(1, len(layer_dims)):\n            self.parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], \n                                                       layer_dims[l-1])*np.sqrt(2./layer_dims[l-1])\n            self.parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n    \n    # define getters and setters for accessing the model class attributes:        \n    def get_layer_dims(self):\n        return self.layer_dims\n    def get_num_layers(self):\n        return self.num_layers\n    def get_params(self, key):\n        return self.parameters.get(key)\n    def set_params(self, key, value):\n        self.parameters[key] = value\n    \n        \n    def forward_propagation(self, X, keep_prob):\n        \"\"\" \n        Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation\n        Arguments:\n            X -- data, numpy array of shape (input size, number of examples)\n            keep_prob - probability of keeping a neuron active during drop-out, scalar\n        Returns:\n            AL -- last post-activation value\n            caches -- list of caches containing:\n               every cache of layers w/ReLU activation (there are L-1 of them, indexed from 0 to L-2)\n               the cache of the output layer with Softmax activation (there is one, indexed L-1)\n        \"\"\"\n        caches = []\n        L = self.get_num_layers()-1    # number of layers with weights (hidden + output)\n        A = X                          # set input as the first hidden layer activation\n        # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n        for l in range(1, L):\n            A_prev = A                # initialize activation of the previous layer\n            W, b = self.get_params(f'W{l}'), self.get_params(f'b{l}') # get weights and biases\n            Z = W.dot(A_prev) + b     # linear activation for the hidden layers\n            A = np.maximum(0,Z)       # ReLU activation for the hidden layers\n            if keep_prob == 1:        # if no dropout\n                cache = (A_prev, Z)   # useful during backpropagation\n            elif keep_prob < 1:       # if dropout is used for regularization\n                D = np.random.rand(A.shape[0], A.shape[1])  # initialize matrix D\n                D = D < keep_prob   # convert entries of D to 0/1 (using keep_prob as threshold)\n                A *= D              # shut down some neurons of A\n                A /= keep_prob      # scale the value of neurons that haven't been shutdown\n                cache = (A_prev, Z, D)   # useful during backpropagation\n            caches.append(cache)\n        # Implement LINEAR -> SOFTMAX. Add \"cache\" to the \"caches\" list.\n        W, b = self.get_params(f'W{L}'), self.get_params(f'b{L}')\n        Z = W.dot(A) + b                        # Linear activation of the output layer\n        Z -= np.max(Z, axis=0, keepdims=True)   # Normalize Z to make Softmax stable\n        AL = np.exp(Z)/np.sum(np.exp(Z),axis=0,\n                              keepdims=True) # Softmax activation of the output layer\n        cache = (A, Z)                          # useful during backpropagation\n        caches.append(cache)\n        return AL, caches\n        \n    \n    def compute_cost(self, AL, Y, lambd):\n        \"\"\"\n        Implement the cost function with L2 regularization.\n        Arguments:\n            AL -- post-activation, output of forward propagation,\n                                    of shape (output size, number of examples)\n            Y -- \"true\" labels vector, of shape (output size, number of examples)\n            lambd -- regularization hyperparameter, scalar\n        Returns:\n            cost - value of the regularized loss function\n        \"\"\"\n        m = AL.shape[1]               # number of training examples\n        L = self.get_num_layers()-1   # number of layers with weights (hidden and output)\n        assert(Y.shape == AL.shape)\n        # Compute the cross-entropy part of the cost for Softmax activation function:\n        cross_entropy_cost = -(1./m) * np.sum(np.multiply(Y, np.log(AL)))\n\n        # Compute L2 regularization cost\n        L2_reg_cost = 0\n        if lambd == 0:\n            pass\n        else:\n            for l in range(1, L+1):     # sum of all squared weights\n                L2_reg_cost += (1./m)*(lambd/2)*(np.sum(np.square(self.get_params(f'W{l}'))))\n\n        # Total cost:\n        cost = cross_entropy_cost + L2_reg_cost\n        # To make sure cost's shape is what we expect (e.g. this turns [[17]] into 17).\n        cost = np.squeeze(cost)   \n        return cost\n    \n            \n    def backward_propagation(self, AL, Y, caches, lambd, keep_prob):\n        \"\"\" \n        Implement the backward propagation for the [LINEAR->RELU]*(L-1)->[LINEAR->SOFTMAX]\n        Arguments:\n            AL -- probability vectors for the training examples, output of the forward propagation\n            Y -- true one-hot \"label\" vectors for the training examples\n            caches -- list of caches containing:\n                every cache of forward propagation with \"relu\" \n                                        (there are (L-1) or them, indexes from 0 to L-2)\n                the cache of forward propagation with \"softmax\" (there is one, index L-1)\n            lambd -- lambda, an L2 regularization parameter, scalar\n            keep_prob - probability of keeping a neuron active during drop-out, scalar\n        Returns: grads -- a dictionary with updated gradients\n        \"\"\"\n        L = self.get_num_layers()-1  # number of layers with weights (hidden and output)\n        m = AL.shape[1]              # number of training examples\n        assert(Y.shape == AL.shape)\n        grads = {}                   # a dict for the gradients of the cost function\n\n        # Lth layer (SOFTMAX -> LINEAR) gradients.\n        W = self.get_params(f\"W{L}\") # get weights for the output layer\n        A_prev, Z = caches[L-1]  # get inputs and linear activations for the output layer\n        dZ = AL - Y                  # Gradient of the cost w.r.t. Z (from calculus)\n        dW = (1./m) * dZ.dot(A_prev.T) + (lambd/m) * W  # Gradient of cost w.r.t. W\n        db = (1./m) * np.sum(dZ, axis=1, keepdims=True) # Gradient of cost w.r.t. b\n        dA_prev = np.dot(W.T,dZ)                        # Gradient of cost w.r.t. dA_prev\n        # Update the grads dictionary for the output layer\n        grads[f\"dW{L}\"], grads[f\"db{L}\"] = dW, db\n\n        # l-th hidden layer: (RELU -> LINEAR) gradients.\n        for l in reversed(range(1, L)):\n            W = self.get_params(f\"W{l}\") # get weights for the l-th hidden layer\n            dA = dA_prev\n            if keep_prob == 1:           # without dropout\n                A_prev, Z = caches[l-1]  # get ReLU & linear activations for l-th hidden layer\n            elif keep_prob < 1:          # with dropout\n                A_prev, Z, D = caches[l-1]  # get ReLU & linear activations + mask\n                dA *= D                     # apply mask to shut down the same neurons as in f.p.\n                dA /= keep_prob             # scale the value of the remaining neurons\n            dZ = np.array(dA, copy=True)    # just converting dz to a correct object.\n            dZ[Z <= 0] = 0                  # when z <= 0, set dz to 0 as well. \n            dW = (1./m) * dZ.dot(A_prev.T) + (lambd/m) * W  # Gradient of cost w.r.t. W\n            db = (1./m) * np.sum(dZ, axis=1, keepdims=True) # Gradient of cost w.r.t. b\n            dA_prev = np.dot(W.T,dZ)          # Gradient of cost w.r.t. dA_prev\n            # Update the grads dictionary for the hidden layers\n            grads[f\"dA{l}\"] = dA\n            grads[f\"dW{l}\"], grads[f\"db{l}\"] = dW, db\n        return grads\n    \n        \n    def update_parameters_with_gd(self, grads, lr):\n        \"\"\" \n        Update parameters of the model using method gradient descent\n        Arguments:\n            grads -- python dictionary containing gradients, output of backprop\n            lr -- the learning rate, scalar\n        Returns: \n            updated parameters (weithts and biases of the model)\n        \"\"\"\n        L = self.get_num_layers()-1  # get number of layers with weights (hidden + output)\n        for l in range(1, L+1):\n            self.set_params(f\"W{l}\", self.get_params(f\"W{l}\") - lr * grads[f\"dW{l}\"])\n            self.set_params(f\"b{l}\", self.get_params(f\"b{l}\") - lr * grads[f\"db{l}\"])\n               \n            \n    def initialize_adam(self):\n        \"\"\"\n        Initializes v and s for the Adam optimizer as two python dictionaries with:\n            keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n            values: numpy arrays of zeros of the same shape as the corresponding gradients.\n        Returns: \n            v -- python dict that will contain the exponentially weighted average of the gradient.\n            s -- python dict that will contain the exponentially weighted average of the squared gradient.\n        \"\"\"\n        v = {}   \n        s = {}\n        L = self.get_num_layers()-1   # get number of layers with weights (hidden and output)\n        for l in range(1, L+1):\n            v[f\"dW{l}\"] = np.zeros((self.get_params(f\"W{l}\").shape))\n            v[f\"db{l}\"] = np.zeros((self.get_params(f\"b{l}\").shape))\n            s[f\"dW{l}\"] = np.zeros((self.get_params(f\"W{l}\").shape))\n            s[f\"db{l}\"] = np.zeros((self.get_params(f\"b{l}\").shape))    \n        return v,s\n    \n    \n    def update_parameters_with_adam(self, grads, v, s, t, lr, beta1, beta2, epsilon):\n        \"\"\"\n        Update parameters of the model using Adam optimization algorithm\n        Arguments:\n            v -- Adam variable, moving average of the first gradient, python dict\n            s -- Adam variable, moving average of the squared gradient, python dict\n            t -- current timestep (minibatch)\n            lr -- the learning rate, scalar.\n            beta1 -- Exponential decay hyperparameter for the first moment estimates \n            beta2 -- Exponential decay hyperparameter for the second moment estimates \n            epsilon -- hyperparameter preventing division by zero in Adam updates\n        Returns:\n            updated parameters (model attributes)\n            v -- Adam variable, moving average of the first gradient, python dict\n            s -- Adam variable, moving average of the squared gradient, python dict\n        \"\"\"\n        v_corr = {}       # Initializing a bias-corrected first moment\n        s_corr = {}       # Initializing a bias corrected second moment estimate\n        L = self.get_num_layers()-1   # get number of layers with weights (hidden+output)\n        \n        # Perform Adam update on all parameters\n        for l in range(1, L+1):\n            # Moving average of the gradients\n            v[f\"dW{l}\"] = beta1*v[f\"dW{l}\"] + (1-beta1)*grads[f\"dW{l}\"]\n            v[f\"db{l}\"] = beta1*v[f\"db{l}\"] + (1-beta1)*grads[f\"db{l}\"]\n\n            # Compute bias-corrected first moment estimate\n            v_corr[f\"dW{l}\"] = v[f\"dW{l}\"]/(1-beta1**t)\n            v_corr[f\"db{l}\"] = v[f\"db{l}\"]/(1-beta1**t)\n\n            # Moving average of the squared gradients\n            s[f\"dW{l}\"] = beta2*s[f\"dW{l}\"] + (1-beta2)*grads[f\"dW{l}\"]**2\n            s[f\"db{l}\"] = beta2*s[f\"db{l}\"] + (1-beta2)*grads[f\"db{l}\"]**2\n\n            # Compute bias-corrected second raw moment estimate\n            s_corr[f\"dW{l}\"] = s[f\"dW{l}\"]/(1-beta2**t)\n            s_corr[f\"db{l}\"] = s[f\"db{l}\"]/(1-beta2**t)\n\n            # Update parameters\n            temp_W = v_corr[f\"dW{l}\"]/(s_corr[f\"dW{l}\"]**0.5 + epsilon)\n            temp_b = v_corr[f\"db{l}\"]/(s_corr[f\"db{l}\"]**0.5 + epsilon)\n            self.set_params(f\"W{l}\", self.get_params(f\"W{l}\") - lr*temp_W)\n            self.set_params(f\"b{l}\", self.get_params(f\"b{l}\") - lr*temp_b)\n\n        return v, s\n        \n        \n    def predict(self, X):\n        \"\"\" \n        This method is used to predict the results of a L-layer neural network.\n        Arguments:\n            X -- dataset of examples to label, shape (num_pixels, num_examples)\n        Returns:\n            p -- predictions of one-hot labels for X, shape (num_classes, num_examples)\n        \"\"\"\n        m = X.shape[1]          # number of examples\n        # Forward propagation\n        AL, _ = self.forward_propagation(X, keep_prob=1)\n        n = AL.shape[0]         # number of classes\n        p = np.zeros((n, m))    # initialize predictions\n        # convert probabilities AL to one-hot label predictions:\n        for i in range(m):\n            max_i = np.amax(AL[:,i], axis=0)\n            for j in range(n):\n                if AL[j,i] == max_i:\n                    p[j,i] = 1\n                else:\n                    p[j,i] = 0\n        return np.int64(p)\n    \n        \n    def fit(self, X, Y, batch_size=128, num_epochs=20, lr=1e-3, min_lr=1e-8, lambd=0, keep_prob=0.75,\n            beta1=0.9, beta2=0.999, epsilon=1e-8, optimizer='adam', print_cost=False):\n        \"\"\" \n        Implements an L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX.\n        Arguments:\n            X -- data, numpy array of shape (# pixels, # examples)\n            Y -- true \"label\" vector, of shape (# classes, # examples)\n            batch_size -- the size of a mini batch on which parameters are get updated\n            num_epochs -- number of epochs, i.e. passes through the training set\n            lr -- learning rate of the gradient descent update rule\n            min_lr -- the lower threshold of the learning rate decay\n            lambd -- lambda, the L2 regularization hyperparameter\n            optimizer -- optimization metod [\"gd\"=gradient_descent or \"adam\"]\n            beta1 -- exp decay hyperparameter for the past gradients estimates in 'adam'\n            beta2 -- exp decay hyperparameter for the past squared gradients estimates in 'adam'\n            epsilon -- hyperparameter preventing division by zero in 'adam' updates\n            print_cost -- if True, it prints the cost every # steps\n        Returns:\n            parameters -- parameters learnt by the model. They can then be used to predict.\n        \"\"\"\n        costs = []                # to keep track of the cost\n        lr_0 = lr                 # to fix the initial learning rate before decay\n        t = 0                     # initializing the minibatch counter (for Adam update)\n        cost_prev = 1000          # initialize cost to a big number\n        # Initialize the optimizer\n        if optimizer == \"gd\":\n            pass                  # no initialization required for gradient descent\n        elif optimizer == \"adam\":\n            v, s = self.initialize_adam()\n        # Optimization loop\n        for epoch in range(num_epochs):\n            # Define the random minibatches. Reshuffle the dataset after each epoch\n            minibatches = random_mini_batches(X, Y, batch_size)\n            # Initialize a list of minibatch costs for each epoch:\n            minibatch_costs = []\n            # update parameters after each minibatch:\n            for minibatch in minibatches:\n                # Select a minibatch\n                (minibatch_X, minibatch_Y) = minibatch\n                # Forward propagation: [LINEAR->RELU]*(L-1) -> LINEAR->SOFTMAX.\n                AL, caches = self.forward_propagation(minibatch_X, keep_prob)\n                # Compute cost.\n                cost = self.compute_cost(AL, minibatch_Y, lambd)\n                minibatch_costs.append(cost)\n                # Backward propagation.\n                grads = self.backward_propagation(AL, minibatch_Y, caches, lambd, keep_prob)\n                # Update parameters\n                t = t + 1 # minibatch counter\n                if optimizer == \"gd\":\n                    self.update_parameters_with_gd(grads, lr)\n                elif optimizer == \"adam\":\n                    v, s = self.update_parameters_with_adam(grads, v, s, t, lr, \n                                                            beta1, beta2, epsilon)\n            # Find average cost over all minibathes within the epoch: \n            ave_cost = sum(minibatch_costs) / len(minibatch_costs)\n            # Define learning rate decay:\n            if ave_cost > cost_prev and lr > min_lr:\n                lr = lr / 2   # reduce lr, but not below the min value\n            cost_prev = ave_cost  # save cost value for the next epoch\n                \n            # Print the cost every 2 epoch\n            if print_cost and epoch % 2 == 0:\n                print(f\"Cost after epoch {epoch}: {ave_cost}\")\n            if print_cost and epoch % 1 == 0:    \n                costs.append(ave_cost)\n        # plot the Learning curve\n        if print_cost:\n            plt.plot(np.squeeze(costs))\n            plt.ylabel(\"cost\")\n            plt.xlabel(\"epochs (x 1)\")\n            plt.title(f\"Learning rate = {lr_0} / {lr}\")\n            plt.show()",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "83a4cde7a4f46ae7a3ac924bdc584a434e425c9c"
      },
      "cell_type": "markdown",
      "source": "## 4. MODEL TRAINING"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8d8743a90ed9f9bf593386bdc41c338987cb2491"
      },
      "cell_type": "code",
      "source": "# Create list to save journal records of the current session\nrecords_list = []",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1831854f8d5da69d4f7351b42df457e17dd13cb3"
      },
      "cell_type": "code",
      "source": "# Create an instance of the Custom_model class with 'layer_dims' architecture:\nc_model = Custom_model(layer_dims)\n\n# Set hyperparameters that we want to tune for the custom model:\nlr          = 1e-3   # the learning rate for the gradient descent\nmin_lr      = 1e-8   # the lower threshold of the learning rate decay\noptimizer   = 'adam'\nbatch_size  = 128  \nnum_epochs  = 25          \nlambd       = 0   # lambda - regularization hyperparameter, scalar\nkeep_prob   = 0.75   # keep_prob - probability of keeping a neuron active during dropout\n\n# Train the model at various hyperparameters settings:\nc_model.fit(X_train, Y_train, batch_size=batch_size, num_epochs=num_epochs,\n            lr=lr, min_lr=min_lr, lambd=lambd, keep_prob=keep_prob,\n            optimizer=optimizer, print_cost=True)       \n\n# Evaluation on the train data:\npredict_train = c_model.predict(X_train)\ncorrect_train = np.argmax(predict_train, axis=0) == np.argmax(Y_train, axis=0)\nc_train_acc = round(np.sum(correct_train)/m_train, 2)\n\n# Evaluation on the validation data:\npredict_val = c_model.predict(X_val)\ncorrect_val = np.argmax(predict_val, axis=0) == np.argmax(Y_val, axis=0)\nc_val_acc = round(np.sum(correct_val)/m_val, 2)\n\nprint(f\"c_model: train accuracy: {c_train_acc * 100}%\")\nprint(f\"c_model: val accuracy = {c_val_acc * 100}%\")\nprint(f'c_model: val error = {round((1 - c_val_acc) * m_val)} examples')\n\n# Update the journal of hyperparameters tuning records\nrecord = {'layer_dims'   : c_model.get_layer_dims(), \n          'acc_train'    : c_train_acc, \n          'acc_val'      : c_val_acc,\n          'val_error'    : round((1 - c_val_acc) * m_val),\n          'batch_size'   : batch_size,\n          'num_epochs'   : num_epochs,\n          'lr'           : lr,\n          'min_lr'       : min_lr,\n          'lambda'       : lambd,\n          'keep_prob'    : keep_prob}\n\nrecords_list.append(record)\njournal = pd.DataFrame(records_list)   # saves records when repeatedly running the current cell\njournal",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Cost after epoch 0: 0.3492630742124156\nCost after epoch 2: 0.10833726246710822\nCost after epoch 4: 0.06554761884331028\nCost after epoch 6: 0.04376762728694768\nCost after epoch 8: 0.03295691296832323\nCost after epoch 10: 0.022390305388455028\nCost after epoch 12: 0.016519263568012553\nCost after epoch 14: 0.015082968834750729\nCost after epoch 16: 0.006938219910223948\nCost after epoch 18: 0.006235288572839235\nCost after epoch 20: 0.005478225286833474\nCost after epoch 22: 0.005293484532983706\nCost after epoch 24: 0.004553626659254887\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "c_model: train accuracy: 100.0%\nc_model: val accuracy = 98.0%\nc_model: val error = 84.0 examples\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "   acc_train  acc_val    ...      num_epochs  val_error\n0        1.0     0.98    ...              25       84.0\n\n[1 rows x 10 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>acc_train</th>\n      <th>acc_val</th>\n      <th>batch_size</th>\n      <th>keep_prob</th>\n      <th>lambda</th>\n      <th>layer_dims</th>\n      <th>lr</th>\n      <th>min_lr</th>\n      <th>num_epochs</th>\n      <th>val_error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>0.98</td>\n      <td>128</td>\n      <td>0.75</td>\n      <td>0</td>\n      <td>[784, 512, 10]</td>\n      <td>0.001</td>\n      <td>1.000000e-08</td>\n      <td>25</td>\n      <td>84.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "ea90786fab38d2045db67a32cc44389a535572ba"
      },
      "cell_type": "markdown",
      "source": "## Conclusion: we have looked \"under the hood\" of a fully connected network with ReLU activation and dropout regularization for the hidden layers, Softmax activation for the output layer, cross-entropy cost with L2 regularization, and parameters update with Adam optimization. \n\n## Great! We know how it works!"
    },
    {
      "metadata": {
        "_uuid": "5ef41a4497e1ab3d9e48e3bda46b696aac199468"
      },
      "cell_type": "markdown",
      "source": "## 5. SAVING THE TRAINED MODELS "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "03c567fa2703ec725cbc082a2f9dc670bc32c230"
      },
      "cell_type": "code",
      "source": "# Save the Keras model to JSON:\nmodel_json = k_model.to_json()\nwith open(\"k_model.json\", \"w\") as f1:\n    f1.write(model_json)\n# Save the weights to HDF5:\nk_model.save_weights(\"k_weights.h5\")",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "55da9e47e7833251b486a0e41e92ff0b980f7890"
      },
      "cell_type": "code",
      "source": "# Save the Custom model weights:\nimport pickle\nwith open(\"c_weights.pickle\", \"wb\") as f2:\n    pickle.dump(c_model.parameters, f2, pickle.HIGHEST_PROTOCOL)",
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "18fba4fb764536a58f7dfdca196468e161e3e245"
      },
      "cell_type": "markdown",
      "source": "## -1. SUBMIT PREDICTIONS TO KAGGLE"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "839c4e62066c4de54fae39f8c74db7964e911d5f"
      },
      "cell_type": "code",
      "source": "# Predict labels on the test dataset:\npredictions = k_model.predict_classes(test_pixels)\n#predict_test = c_model.predict(X_test)   # one-hot prediction vectors\n#predictions = np.argmax(predict_test, axis=0)  # label predictions\n\nsubmission = pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                           \"Label\"  : predictions})\nsubmission.to_csv(\"submission.csv\", index=False, header=True)\nsubmission.head()",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "   ImageId  Label\n0        1      2\n1        2      0\n2        3      9\n3        4      9\n4        5      3",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ImageId</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}